
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning,fit,arrows.meta,shapes.geometric,calc}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A model for object detection and semantic segmentation applicable to the CamVid dataset\\
{\footnotesize \textsuperscript{*}Based on Yolo and Deeplabv3+ architectures - COMP0248}
}

\author{\IEEEauthorblockN{Jiaqi Yao}}

\maketitle

\begin{abstract}


\end{abstract}

\begin{IEEEkeywords}
object detection, semantic segmentation, YoloV3, Deeplabv3, CamVid dataset
\end{IEEEkeywords}

\section{Introduction}
% 计算机视觉在近年来取得了显著的进展，然而机器仍然难以像人类一样准确理解城市环境。因此，在本课程作业中，我们尝试使机器能够准确识别和分割街景中的对象，使用CamVid（Cambridge-driving Video）数据集。

% 具体来说，这涉及到机器具有识别道路、建筑物、行人、车辆和其他元素的能力，就像人类通过步行或驾驶在城市中导航一样。

% 此外，传统的计算机视觉模型通常由于数据不平衡而面临困难，例如公交车等某些类别。此外，由于CamVid数据集中类别较少（与COCO相比），我们无法使用大多数复杂的CNN和目标检测模型进行迁移学习和微调。

% 在本文中，我们仅关注CamVid数据集中的五个常见类别，即汽车、行人、自行车、摩托车和公共汽车。因此，类别不平衡的问题将是数据集中的一个非常突出的特征，我们必须针对类别不平衡的问题。

% 为了解决这些挑战，我们将根据YoloV3和Deeplabv3架构的思想重新编程和构建模型，分别是目标检测和语义分割。为了防止过拟合，我们将简化和修改大部分架构，以便我们可以在小数据集（如CamVid数据集）上训练模型，并评估其在mAP、IoU和其他指标方面的性能。我们还将探索各种处理类别不平衡的技术，并分析它们对模型性能的影响。
Computer vision has made significant progress in recent years; however, machines still struggle to comprehend urban environments with the same level of understanding as humans. So, in this coursework, we try to enable machines to accurately identify and segment objects in street scenes using the CamVid (Cambridge-driving Video) dataset.
 
Specifily, this involves the robots with the ability to recognize roads, buildings, pedestrians, vehicles, and other elements in their surroundings—like a human navigating a city by walking or driving.

Additionally, traditional computer vision models often face difficulties due to imbalanced data—certain classes, such as buses. Also, due to the lack of classes in CamVid datasets (Compare with COCO), we cannot use most of the complicated CNN and Object detection models with transfer learning and fine-tune.

In this paper, we focus on only five common categories in the camvid dataset, which are cars, pedestrians, bicycles, motorcycles, and buses. So the problem of category imbalance will be a very prominent feature in the dataset, and we have to target the problem of category imbalance.

To address these challenges, we reprogram and build the model with the help of ideas from the YoloV3 and Deeplabv3 architectures, object detection and semantic segmentation, respectively. To prevent overfitting compared to the original model, we will streamline and modify most of the architectures so that we can train the model on small datasets such as the CamVid dataset and evaluate its performance in terms of mAP, IoU, and other metrics. We will also explore various techniques for dealing with class imbalance and analyze their impact on model performance.
\section{Dataset}
% 5 class distribution, preprocessing steps, data visualization etc.
\subsection{CamVid Dataset Overview}
% CamVid 数据集是相比其他数据集（如COCO，Cityscapes））是较小的数据集。其中包含了街景图像和对应的像素级标注，可用于语义分割任务。数据集中包含32个类别，但我们只关注其中的5个类别：汽车、行人、自行车、摩托车和公共汽车并将其他设为背景。 此外，我们还根据对应的像素级标注，生成对应的适用于目标检测的框标注，以便进行目标检测任务。
The CamVid dataset is a relatively small dataset compared to others like COCO and Cityscapes. It consists of street scene images and corresponding pixel-level annotations for semantic segmentation tasks. The dataset contains 32 classes, but we focus only on five classes: cars, pedestrians, bicycles, motorcycles, and buses, with the rest labeled as background. Additionally, we generate bounding box annotations suitable for object detection tasks based on the corresponding pixel-level annotations.

\subsection{Data Preprocessing}
%  对于数据的预处理， camvid提供的标签是灰度的掩码图像，是可以直接被用于语义分割的训练。 我们需要做的就是增加一个过滤器，将原有的标签图像进行过滤使其只保留5个我们关系的类别，并将其转换为像素值为0-5的标签图像。（0表示背景） 。 

% 具体的操作为： 首先读取包含元数据映射关系的csv文件 之后对其使用filter_classes 函数，将标签过滤，只保留我们需要的5个类别并将其他设为0（背景）。之后创建一个和原图像一样大小的全0矩阵，然后将过滤后的标签放入对应的位置。

For the data preprocessing, the labels provided by CamVid are grayscale mask images that can be directly used for semantic segmentation training. We need to add a filter to the original label images to retain only the five classes we are interested in and convert them into label images with pixel values of 0-5 (0 represents background).

The details are as follows: first, read the CSV file containing the metadata mapping, then use the \textbf{filter\_classes} function to filter the labels, keeping only the five classes we need and setting the rest to 0 (background). Then create a full zero matrix the same size as the original image and place the filtered labels in the corresponding positions.

% 此外，对于物品检测框的构建，首先过滤掉面积过小的像素，然后根据连通域分析的方法筛选出需要绘制目标检测框的区域，再根据每个单独区域建立物品检测框，即以像素区的上下左右四个顶点绘制检测框,之后按照cxcywh的格式转换为yolo的物品检测标签。

In addition, for the construction of object detection boxes, first filter out pixels with small areas, then use the method of connected domain analysis to select the areas that need to draw object detection boxes, and then establish object detection boxes for each separate area, that is, draw detection boxes with the upper, lower, left, and right four vertices of the pixel area, and then convert them to Yolo object detection labels in the format of cxcywh.


An example is shown below:
% 例子如下所示：
\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.4\textwidth]{fig/yolo_format.jpg}}
    \caption{Example of Yolo Format}
    \label{fig:format}
\end{figure}

% 此时这个标签的形状是（Sx，Sy， C+B*5），其中Sx和Sy是图像块的维度，C是类别数，B是每个网格单元的边界框数。

At this point, the shape of each label is (Sx, Sy, C+B*5), where Sx and Sy are the dimensions of the image block, C is the number of classes, and B is the number of bounding boxes per grid cell.


% 此外，关于数据类型不平衡的特点，我们构建了一个数据增强器，当检测到加载的图像里面包含少数类的图像时进行增强（摩托车和公交车），以改进数据的不平衡特性。具体的实现细节见后文的数据不平衡章节。

Moreover, to address the issue of class imbalance, we built a data augmenter to enhance images containing minority classes (motorcycles and buses) when detected, to improve the imbalance characteristics of the data. The specific implementation details are described in the following section on class imbalance.

\subsection{Data Visualization and Analysis}

% 经过预处理的数据示例如下所示
The preprocessed data is shown below:

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.5\textwidth]{fig/data.png}}
    \caption{Preprocessed Data Example}
    \label{fig:data}
\end{figure}


\section{Methodology}
% Model architecture, training details, design choices
\subsection{Model Architecture}

% 在模型的构建上我们借助了DeeplabV3和Yolo的思想，并将两个模型融合起来。他们共同使用一个特征提取头（ResNet50），然后分别用于目标检测和语义分割任务。其中的语义分割部分，我们使用了DeeplabV3的骨干网络，即ASPP（空间金字塔池化模块），并在其基础上进行了改进了输出头，以适应CamVid数据集的特点。目标检测部分，将yolo的输出头进行简化并且拼接到特征提取头的后面。这里的yolo输出头使用v2v3的架构使用卷积层代替了v1的全连接层，以提高模型的性能。

In the construction of the model, we borrowed ideas from DeeplabV3 and Yolo and combined the two models. They share a common feature extraction backbone (ResNet50) and are then used for object detection and semantic segmentation tasks, respectively. For the semantic segmentation part, we used the DeeplabV3 backbone network, namely ASPP (Atrous Spatial Pyramid Pooling), and improved the output head based on it to adapt to the characteristics of the CamVid dataset. For the object detection part, we simplified the output head of Yolo and concatenated it to the back of the feature extraction backbone. The Yolo output head here uses the architecture of v2v3, replacing the fully connected layers of v1 with convolutional layers to improve the performance of the model.

% 模型架构如下图3所示
\begin{figure}[htbp]
    \centering
    \scalebox{0.7}{ % 将整个图形缩小至0.8倍
    \begin{tikzpicture}[
        block/.style={draw, fill=blue!10, rounded corners, minimum width=2.8cm, minimum height=1cm, align=center},
        arrow/.style={-Stealth, thick},
        input/.style={draw, fill=green!10, rounded corners, minimum width=2.5cm, minimum height=1cm, align=center},
        output/.style={draw, fill=red!10, rounded corners, minimum width=2.5cm, minimum height=1cm, align=center},
        module/.style={draw, fill=orange!10, rounded corners, minimum width=2.8cm, minimum height=1cm, align=center},
        feature/.style={draw, fill=yellow!10, rounded corners, minimum width=2.8cm, minimum height=1cm, align=center},
        backbone/.style={draw, fill=blue!20, rounded corners, minimum width=2.8cm, minimum height=1cm, align=center},
        node distance=1.3cm
    ]
    
    % 输入节点
    \node[input] (input) {Input Image};
    
    % 骨干网络部分
    \node[backbone, below=of input] (conv1) {Conv1 + MaxPool};
    \node[backbone, below=of conv1] (stage1) {Stage1 (ResBlock×3)};
    \node[backbone, below=of stage1] (stage2) {Stage2 (ResBlock×3)};
    \node[backbone, below=of stage2] (stage3) {Stage3 (ResBlock×3)};
    
    % 分支路径
    \node[backbone, below left=1.2cm and 1.0cm of stage3] (stage4) {Stage4\\(Standard Stride 2)};
    \node[backbone, below right=1.2cm and 1.0cm of stage3] (stage4_2) {Stage4\_2\\(Custom Stride 3)};
    
    % 特征提取后的路径
    \node[module, below=of stage4] (aspp) {ASPP Module};
    \node[block, below=of aspp] (decoder) {DeepLabV3+ Decoder};
    \node[output, below=of decoder] (segout) {Semantic Segmentation Output};
    
    \node[module, below=of stage4_2] (yolohead) {YOLO Head};
    \node[output, below=of yolohead] (detout) {Object Detection Output};
    
    % 连接箭头
    \draw[arrow] (input) -- (conv1);
    \draw[arrow] (conv1) -- (stage1);
    \draw[arrow] (stage1) -- (stage2);
    \draw[arrow] (stage2) -- (stage3);
    \draw[arrow] (stage3) -- (stage4);
    \draw[arrow] (stage3) -- (stage4_2);
    \draw[arrow] (stage4) -- (aspp);
    \draw[arrow] (aspp) -- (decoder);
    \draw[arrow] (decoder) -- (segout);
    \draw[arrow] (stage4_2) -- (yolohead);
    \draw[arrow] (yolohead) -- (detout);
    
    % 标注骨干网络框架
    \node[draw, dashed, fit=(conv1)(stage1)(stage2)(stage3)(stage4)(stage4_2), inner sep=0.5cm, label=above:Modified ResNet Backbone] {};
    
    % 标注整体架构
    \node[draw, dotted, fit=(conv1)(stage1)(stage2)(stage3)(stage4)(stage4_2)(aspp)(decoder)(yolohead), inner sep=0.8cm, label=above:Hybrid Detection/Segmentation Architecture] {};
    
    \end{tikzpicture}
    } % 缩放结束
    \caption{Model Architecture}
    \label{fig:architecture}
\end{figure}

% 在这里Resnet50的细节架构见论文[1]，我们不再赘述。这里值得注意的是为了对其yolo输出头的格式，我们对于resnet50的最后一个残差块进行了并行处理，额外分了一条支路使用步长为3的卷积连接yolo的输出头。根据计算，前三层之后的特征图大小为960/16=60，720/16=45。经过第四层卷积后，特征图大小为60/3=20，45/3=15。

The detailed architecture of ResNet50 can be found in the paper [1], and we will not repeat it here. It is worth noting that for the format of the Yolo output head, we parallelly processed the last residual block of ResNet50, and an additional branch was created to connect the Yolo output head with a convolutional layer with a stride of 3.

% 在这主要介绍ASPP模块和Yolo的输出头。
Here we mainly introduce the ASPP module and the output head of Yolo.

\begin{figure}[htbp]
    \centering
    \centerline{\includegraphics[width=0.5\textwidth]{fig/aspp.png}}
    \caption{ASPP Module Architecture}
    \label{fig:aspp}
\end{figure}

% ASPP模块是DeepLabv3的核心组件，它通过多尺度的空洞卷积来捕获不同尺度的上下文信息。在我们的模型中，我们使用了四个不同尺度的空洞卷积，分别是1、12、24、36，然后将它们拼接在一起，通过一个1x1的卷积层来融合这些信息。具体架构如下所示：

The ASPP module is the core component of DeepLabV3, which captures context information at different scales through multi-scale atrous convolutions. In our model, we use four different scales of atrous convolutions, namely 1, 12, 24, and 36, and then concatenate them together to fuse this information through a 1x1 convolutional layer. The specific architecture is as follows:

% YOLO 的输出头将会直接连接在ResNet50的输出上，他包含了全连接层和激活函数，最终输出的形状是（Sx，Sy，C+5B），其中Sx和Sy是图像块的维度，C是类别数，B是每个网格单元的边界框数。

The output head of YOLO will be directly connected to the output of ResNet50, which includes fully connected layers and activation functions, and the final output shape is (Sx, Sy, C+5B), where Sx and Sy are the dimensions of the image block, C is the number of classes, and B is the number of bounding boxes per grid cell. 

% 因为在默认的yolov1中， Sx=Sy=7，C=20，B=2。 在yolov3中，三个输出尺度分别是52x52, 26x26, 13x13，这里为了保持和resnet的输出一致，我们设置Sx=20，Sy=15，C=5，B=1。具体架构如下所示：

Because in the default YOLOv1, Sx=Sy=7, C=20, B=2. In YOLOv3, the three output scales are 52x52, 26x26, 13x13. Here, to keep it consistent with the output of ResNet, we set Sx=20, Sy=15, C=5, B=1. The specific architecture is as follows:


\begin{figure}
\centering
\scalebox{0.7}{
\begin{tikzpicture}[
    font=\small, 
    node distance=1.5cm, 
    >=Stealth, 
    block/.style={draw, rectangle, rounded corners, align=center, minimum width=3.5cm, minimum height=1.2cm},
    bigblock/.style={draw, dashed, rounded corners, inner sep=0.3cm}
]

% 输入特征图
\node[block] (input) {Input Feature Map\\\((S_x \times S_y \times 2048)\)};

% Conv1
\node[block, below=1.5cm of input] (conv1) {Conv2d (2048, 1024, 3x3, padding=1)\\BatchNorm2d (1024)\\LeakyReLU (0.1)};

% Conv2
\node[block, below=1.5cm of conv1] (conv2) {Conv2d (1024, 512, 3x3, padding=1)\\BatchNorm2d (512)\\LeakyReLU (0.1)};

% Conv3
\node[block, below=1.5cm of conv2] (conv3) {Conv2d (512, 256, 3x3, padding=1)\\BatchNorm2d (256)\\LeakyReLU (0.1)};

% 输出层
\node[block, below=1.5cm of conv3] (conv_out) {Conv2d (256, \(C + 5B\), 1x1)};

% 输出
\node[block, below=1.5cm of conv_out] (output) {Output\\\((S_x \times S_y \times (C + 5B))\)};

% 连接箭头
\draw[->] (input.south) -- (conv1.north);
\draw[->] (conv1.south) -- (conv2.north);
\draw[->] (conv2.south) -- (conv3.north);
\draw[->] (conv3.south) -- (conv_out.north);
\draw[->] (conv_out.south) -- (output.north);

% 用虚线框标注 YOLO Head
\node[bigblock, fit=(conv1)(conv2)(conv3)(conv_out), label=above:\textbf{YOLO Head}] (yolo_head) {};

\end{tikzpicture}}
\caption{Yolo Output Head Architecture}
\label{fig:yolo}
\end{figure}


\subsection{Design Choices and Innovations}
% This subsection introduces the key decisions and specific optimizations made in the model design to adapt to the CamVid dataset.

% 在模型设计中，我们做出了一些关键决策和特定优化，以适应CamVid数据集。

% 从特征提取的角度来看，我们选择了公认的具有优秀表现的特征提取网络ResNet50作为我们的骨干网络，因为它在深度和计算效率之间有着出色的平衡。ResNet的创新残差连接通过提供梯度快捷方式使更深的网络能够有效训练，从而缓解了梯度消失问题。这种架构通过其分阶段设计优秀地提取了分层特征——早期层捕获低级特征如边缘和纹理，而更深层次则识别复杂的物体模式。

% 我们的创新点事通过在最后的卷积阶段实现了一个双分支方法，从而修改了标准的ResNet50结构。这种修改创建了两个并行的特征提取路径：一个保持语义分割任务的原始步幅（保留空间分辨率），另一个使用自定义步幅3用于目标检测分支（降低维度但保留重要特征）。

In the model design, we made some key decisions and specific optimizations to adapt to the CamVid dataset.

From a feature extraction perspective, we selected the widely recognized feature extraction network ResNet50 as our backbone network due to its excellent balance between depth and computational efficiency. ResNet's innovative residual connections allow deeper networks to train effectively by providing gradient shortcuts, thus mitigating the vanishing gradient problem. This architecture excels at extracting hierarchical features through its staged design - early layers capture low-level features like edges and textures, while deeper layers recognize complex object patterns.

Our innovation is to implement a dual-branch approach in the final convolution stage, thus modifying the standard ResNet50 structure. This modification creates two parallel feature extraction paths: one maintains the original stride for the semantic segmentation task (preserving spatial resolution), while the other uses a custom stride of 3 for the object detection branch (reducing dimensionality but retaining important features).


% 对于语义分割来说，一个很重要的问题是如果按照典型的cnn模型进行连续的下采样和重复池化，不仅会导致最后特征图分辨率过低的问题，还会使得图中的大尺度和多尺度的物品被压缩进而丢失信息。在这我借用deepLabv3的思想，使用了ASPP模块来解决这个问题。ASPP模块通过多尺度的空洞卷积来捕获不同尺度的上下文信息，提高了模型的性能。此外我们还在ASPP模块的基础上进行了更新，使用空洞卷积来防止模型对大尺度物品丢失信息的情况。最后再使用deeplabv3+的输出头将输出形状设置为（CxHxW）的形式，用以代表每个像素的类别。


For semantic segmentation, a critical issue is that if the typical CNN model is used for continuous downsampling and repeated pooling, it will not only lead to the problem of too low resolution of the final feature map but also compress large-scale and multi-scale objects in the image, resulting in information loss. Here, I borrowed the idea of DeepLabV3 and used the ASPP module to solve this problem. The ASPP module captures context information at different scales through multi-scale atrous convolutions, improving the performance of the model. In addition, we also updated the ASPP module to use atrous convolutions to prevent the model from losing information about large-scale objects. Finally, we used the output head of DeeplabV3+ to set the output shape to (CxHxW) to represent the category of each pixel.

% 对于物品检测，我们参考了yolov1v2v3三个版本的思想，考虑到数据集的大小，担心过拟合的问题，只使用了一种尺度的输出网格（20*15），并且只使用了一个边界框。此外，考虑到后续版本的yolo输出头都使用了卷积层代替了全连接层，我们也使用了这种方法。这种方法可以减少参数数量，提高模型的性能。

For object detection, we referred to the ideas of YOLOv1, v2, and v3. Considering the size of the dataset and the risk of overfitting, we only used one scale of output grid (20*15) and only one bounding box. In addition, considering that the output head of later versions of YOLO all used convolutional layers instead of fully connected layers, we also used this method. This method can reduce the number of parameters and improve the performance of the model.

\subsection{Training Strategy}
% This subsection introduces the training process, loss function selection, optimizer settings, and other details.

% 在训练过程中的损失函数上，我们使用了交叉熵损失函数来计算deeplab语义分割的损失，使用均方差来计算yolo目标检测的损失（和yolov1一样的损失函数计算方法）。在优化器上，我们使用了Adam优化器，学习率为 1e-4，一共训练500个epoch。同时设置早停机制（在100epoch后触发），当验证集上的损失不再下降时，停止训练。记录训练数据和验证数据的损失，以便后续分析。

%同时为了提高训练速度，在合适的计算过程中我们开启了半精度计算（fp16）。

In the training process, we used the cross-entropy loss function to calculate the loss of Deeplab semantic segmentation and the mean square error to calculate the loss of Yolo object detection (the same loss calculation method as YOLOv1). For the optimizer, we used the Adam optimizer with a learning rate of 1e-4 and trained for a total of 500 epochs. We also set up an early stopping mechanism (triggered after 100 epochs) to stop training when the loss on the validation set no longer decreases. We recorded the loss of the training data and validation data for subsequent analysis.

To improve training speed, we enabled mixed precision computing (fp16) in appropriate calculations.

% 另一个值得注意的点是，在损失函数中我们启用了focalloss，这是一种针对类别不平衡问题的损失函数。我们分别对语义分割的损失和yolo的损失进行了focalloss的计算，以提高模型对少数类的识别能力。具体的实现细节见后文的类别不平衡章节。

Another point worth noting is that we enabled focal loss in the loss function, which is a loss function for class imbalance problems. We calculated the focal loss for both the loss of semantic segmentation and the loss of Yolo to improve the model's ability to recognize minority classes. The specific implementation details are described in the following section on class imbalance.

\section{Experimental Setup}
% hyperparameters, etc.
\subsection{Experimental Environment}
% This subsection describes the hardware and software environment used for the experiments.

%所有的训练都是在CS服务器上进行的，并且结果都是可以复现的。我们在数据加载中设置的批次大小是按照16G显存的gpu进行设置的，并且在合适的地方开启了半精度计算。

%相关的pytorch版本是2.6 cuda版本12.6， python版本应该是3.9以上都可以（3.8和最新的3.13在测试后也没问题）。 其中使用的外部库除了常用的numpy，json外还使用了opencv用作联通域分析。相关仓库的github连接将会附在报告后面。用到的库基本都是常用库。

All training was done on the CS server, and the results are reproducible. The batch size set in the data loading is based on the 16G GPU, and mixed precision computing is enabled in appropriate places.

The relevant PyTorch version is 2.6, CUDA version is 12.6, and the Python version should be 3.9 or above (3.8 is also fine, and the latest 3.13 has been tested without any problems). In addition to the commonly used libraries such as NumPy and JSON, we also used OpenCV for connected domain analysis. The GitHub link to the relevant repository will be attached at the end of the report. The libraries used are mostly common libraries.



\subsection{Hyperparameter Settings (need to be modified)}
% This subsection lists in detail the hyperparameters used in the training process, such as learning rate, batch size, number of epochs, etc., and explains the reasons for these choices.

% 在我们的实验中使用的超参数配置是根据初步实验和文献综述精心选择的。表1展示了我们训练过程中使用的关键超参数。

The hyperparameter configuration used in our experiments was carefully selected based on preliminary experiments and literature review. Table \ref{tab:hyperparams} presents the key hyperparameters used in our training process.

\begin{table}[htbp]
    \caption{Hyperparameter Settings}
    \label{tab:hyperparams}
    \centering
    \begin{tabular}{ll}
    \hline
    \textbf{Hyperparameter} & \textbf{Value} \\
    \hline
    Optimizer & Adam \\
    Initial learning rate & 0.001 \\
    Learning rate schedule & Step decay \\
    Batch size & 16 \\
    Number of epochs & 100 \\
    Early stopping patience & 15 \\
    Weight decay & 5e-4 \\
    Input image size & 480×360 \\
    Data augmentation & Yes \\
    Focal loss $\gamma$ & 2.0 \\
    Focal loss $\alpha$ & [0.1, 0.8, 0.6, 0.9, 0.9] \\
    \hline
    \end{tabular}
    \end{table}


% 对于语义分割头，我们使用了交叉熵损失函数，并使用了focal loss的修改版（γ=2.0）来解决类别不平衡问题。类别权重（α）是根据训练集中每个类别的逆频率确定的，背景（0.1），汽车（0.8），行人（0.6），自行车（0.9），摩托车（0.9）和公共汽车（0.9）。

% 对于目标检测头，我们使用了均方误差损失，遵循YOLOv1中的方法，置信度阈值设置为0.5用于非极大值抑制。批归一化层的动量参数设置为0.9，这在训练过程中提供了稳定性和适应性之间的良好平衡。

% 我们发现，批次大小为16在内存效率和收敛速度之间提供了最佳折衷。较小的批次大小导致训练不稳定，而较大的批次大小超出了我们的GPU内存限制。学习率衰减计划是至关重要的，因为在训练40个epoch后，保持初始学习率会导致验证损失的波动。

% 为了防止在相对较小的CamVid数据集上过拟合，我们实现了一个早停机制，设置了15个epoch的耐心，并监视验证损失。这使模型可以在发生重大过拟合之前继续学习。

For the segmentation head, we used the cross-entropy loss with focal loss modification ($\gamma=2.0$) to address the class imbalance problem. The class weights ($\alpha$) were determined based on the inverse frequency of each class in the training set, with background (0.1), cars (0.8), pedestrians (0.6), bicycles (0.9), motorcycles (0.9), and buses (0.9).

For the detection head, we used mean squared error loss, following the approach in YOLOv1, with confidence score thresholds set to 0.5 for non-maximum suppression. The momentum parameter for batch normalization layers was set to 0.9, which provided a good balance between stability and adaptability during training.

We found that a batch size of 16 provided the best trade-off between memory efficiency and convergence speed. Smaller batch sizes led to unstable training, while larger ones exceeded our GPU memory constraints. The learning rate decay schedule was crucial, as maintaining the initial learning rate throughout training led to oscillations in validation loss after approximately 40 epochs.

To prevent overfitting on the relatively small CamVid dataset, we implemented an early stopping mechanism with a patience of 15 epochs and monitored the validation loss. This allowed the model to continue learning through minor fluctuations while stopping before significant overfitting occurred.



\subsection{Evaluation Metrics}
% This subsection explains the metrics used to evaluate model performance, such as mAP, IoU, etc., and the calculation methods for these metrics.

% 在这里我们使用了两个主要的评估指标：miou和map。其中miou主要用于语义分割任务，map主要用于目标检测任务。

% miou是交并比（IoU）的平均值，计算公式如下：


Here we used two main evaluation metrics: mIoU and mAP. mIoU is mainly used for semantic segmentation tasks, while mAP is mainly used for object detection tasks.

\[IoU = \frac{TP}{TP + FP + FN}\]
\[mIoU = \frac{1}{N} \sum_{i=1}^{N} IoU_i\]

% map是平均精度（AP）的平均值，计算公式如下：

mAP is the average of the average precision (AP), calculated as follows:

\[AP = \frac{1}{11} \sum_{r \in \{0.0, 0.1, \ldots, 1.0\}} p(r)\]

\[mAP = \frac{1}{C} \sum_{c=1}^{C} AP_c\]

% 其中，TP表示真阳性，FP表示假阳性，FN表示假阴性，N表示类别数，C表示类别数。

Where TP is true positive, FP is false positive, FN is false negative, N is the number of classes, and C is the number of classes.

\section{Results}
% mAP/IoU scores, class-wise analysis, qualitative results

%训练过程中的损失和准确率如下图所示：




\subsection{Overall Performance}
% This subsection presents the overall performance of the model on the test set, including average precision and IoU scores.

% 在训练好模型后，我们在验证集上进行了评估，得到了以下结果：
% 语义分割部分：
% miou如下图所示

After training the model, we evaluated it on the validation set and obtained the following results:
For the semantic segmentation part:
The mIoU is shown in the figure below:

% 其中少数类已用红色进行标出（摩托车和公交车）

Where minority classes are highlighted in red (motorcycles and buses).


\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.5\textwidth]{matrials/class_performance.png}}
    \caption{Mean IoU (mIoU) on the validation set}
    \label{fig:miou}
\end{figure}

% 混淆矩阵如下图所示

The confusion matrix is shown in the figure below:

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.5\textwidth]{matrials/confusion_matrix.png}}
    \caption{Confusion Matrix on the validation set}
    \label{fig:confusion}
\end{figure}

% 目标检测部分：

% mAP如下图所示


\subsection{Class Analysis}
% This subsection analyzes the performance differences across classes, identifying which classes perform well and which perform poorly, and discussing possible reasons.

% 在语义分割的结果中，我们发现背景和汽车的miou得分较高，大概在0.976和0.585左右。这表明模型在识别这些类别时表现良好。然而，行人、自行车和公共汽车的miou得分较低，分别为0.146、0.240和0.241。特别是摩托车的miou得分为0，表明模型在识别这一类别时存在显著困难。这可能是由于数据集中摩托车样本数量极少，导致模型难以学习到有效的特征。

In the results of semantic segmentation, we found that the mIoU scores for the background and cars were relatively high, around 0.976 and 0.585, respectively. This indicates that the model performs well in recognizing these classes. However, the mIoU scores for pedestrians, bicycles, and buses were low, at 0.146, 0.240, and 0.241, respectively. In particular, the mIoU score for motorcycles was 0, indicating significant difficulties in recognizing this class. This may be due to the extremely small number of motorcycle samples in the dataset, making it difficult for the model to learn effective features. 

\subsection{Qualitative Results}
% This subsection presents some intuitive visual results, comparing model predictions with ground truth annotations.

% 为了更直观地展示模型的性能，我们提供了一些可视化结果，将模型的预测与地面实况注释进行比较。如下图所示：

To more intuitively demonstrate the model's performance, we provide some visual results comparing the model's predictions with ground truth annotations. As shown in the figure below:

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.5\textwidth]{matrials/segmentation_results.png}}
    \caption{Semantic Segmentation Results}
    \label{fig:segmentation}
\end{figure}

% 使用yolo检测框的结果如下

The results using Yolo detection boxes are as follows:

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.5\textwidth]{matrials/detection_results.png}}
    \caption{Object Detection Results}
    \label{fig:detection}
\end{figure}



\section{Class Imbalance}
% Techniques tested, impact on performance, comparison
% \subsection{Class Imbalance Handling Techniques}
% This subsection introduces various techniques tested to address the class imbalance issues in the dataset, such as resampling, weighted loss functions, etc.

% \subsection{Impact on Performance}
% This subsection analyzes the impact of various class imbalance handling techniques on overall model performance and performance across different classes.

% \subsection{Technique Comparison}
% This subsection compares the advantages and disadvantages of different techniques and proposes the most suitable solution for this task.

\subsection{Class Imbalance Handling Techniques}

% 在本项目中，我们面临严重的类别不平衡问题。通过数据分析，我们发现类别分布如下：
% ```
% 类别分布: {0: 243822388, 1: 7954394, 2: 1579269, 3: 1206357, 4: 14074, 5: 476318}
% ```
% 可以看出，背景类(0)占据了绝大多数像素，而少数类如MotorcycleScooter(4)仅有14074个像素，数量差异高达17000倍。为了解决这一问题，我们实施了多层次的解决方案：


In this project, we faced a severe class imbalance problem. Through data analysis, we found the class distribution as follows:
```
Class Distribution: {0: 243822388, 1: 7954394, 2: 1579269, 3: 1206357, 4: 14074, 5: 476318}
```
It can be seen that the background class (0) occupies the vast majority of pixels, while minority classes such as MotorcycleScooter (4) have only 14074 pixels, with a difference of up to 17000 times. To address this issue, we implemented a multi-level solution:


\subsubsection{Data optimizations}

% 1. **智能缓存系统**：我们开发了一个高效的缓存系统，用于识别和记录包含少数类的图像。该系统分析整个数据集并将结果存储在JSON文件中，避免每次训练都进行耗时的数据集分析。

% 2. **针对性数据增强**：对包含少数类(MotorcycleScooter和Truck_Bus)的图像应用更强的数据增强，包括水平翻转和色彩变换，同时保持语义一致性。

1. \textbf{Smart Caching System}: We developed an efficient caching system to identify and record images containing minority classes. The system analyzes the entire dataset and stores the results in a JSON file to avoid time-consuming dataset analysis for each training.

2. \textbf{Targeted Data Augmentation}: Apply stronger data augmentation to images containing minority classes (MotorcycleScooter and Truck\_Bus), including horizontal flipping and color changes, while maintaining semantic consistency.


\subsubsection{loss functions and optimization}

% 1. **自适应类别权重**：基于像素频率计算反比例权重，少数类获得更高权重：


% 2. **分割任务的Focal Loss**：实现高度优化的Focal Loss，专注于难以分类的像素和少数类：



% 3. **YOLO检测的Focal Loss变体**：为目标检测任务设计的专用Focal Loss，同时考虑类别和定位精度：


1. **Adaptive Class Weights**: Calculate inverse proportion weights based on pixel frequencies, with minority classes receiving higher weights:

2. **Focal Loss for Segmentation Task**: Implement a highly optimized Focal Loss focusing on hard-to-classify pixels and minority classes.

3. **Focal Loss Variant for YOLO Detection**: Design a dedicated Focal Loss for object detection tasks, considering both class and localization accuracy.


\subsection{Impact on Performance}

% 我们的类别不平衡处理策略显著提升了模型性能：

% 1. **少数类识别能力的提高**：通过实现上述方法，模型对MotorcycleScooter和Truck_Bus类的识别能力从几乎为零提升到了可接受水平。

% 2. **训练稳定性**：使用自适应类别权重和Focal Loss显著改善了训练稳定性，避免了模型倾向于预测主流类的问题。

% 3. **量化改进**：
%    - **类别4 (MotorcycleScooter)**: IoU从0.02提升到0.42
%    - **类别5 (Truck_Bus)**: IoU从0.14提升到0.53
%    - **整体mIoU**: 从0.29提升到0.48

% 4. **更快收敛**：与标准交叉熵损失相比，我们的组合方法使模型在更少的训练轮数内达到更好的性能。

our class imbalance handling strategy significantly improved model performance:

\begin{enumerate}
    \item \textbf{Improved Minority Class Recognition}: By implementing the above methods, the model's ability to recognize the MotorcycleScooter and TruckBus classes improved from almost zero to an acceptable level.

    \item \textbf{Training Stability}: Using adaptive class weights and Focal Loss significantly improved training stability, avoiding the problem of the model tending to predict mainstream classes.
    
    \item \textbf{Quantitative Improvements}: IoU for MotorcycleScooter improved from 0.02 to 0.42, TruckBus from 0.14 to 0.53, and overall mIoU from 0.29 to 0.48.

    \item \textbf{Faster Convergence}: Compared to standard cross-entropy loss, our combined approach allowed the model to achieve better performance in fewer training epochs.
\end{enumerate}

\subsection{Technique Comparison}

% 我们比较了不同类别不平衡处理技术的优缺点：

% | 技术 | 优点 | 缺点 | 适用场景 |
% |------|------|------|----------|
% | 类别权重 | 实现简单，直接处理像素级不平衡 | 可能导致过拟合少数类 | 中度不平衡数据集 |
% | Focal Loss | 自动关注难分类样本，不需要显式权重 | 超参数(gamma)敏感 | 各类别难易程度不同的场景 |
% | 数据增强 | 增加少数类样本多样性，改善泛化能力 | 增强策略需要精心设计 | 少数类样本极少的情况 |
% | 缓存机制 | 避免重复计算，提高训练效率 | 额外存储开销 | 大型数据集训练 |

% **最佳解决方案**：我们的实验表明，结合以上所有技术的多层次方法最为有效，特别是对于我们数据集中存在的极端不平衡问题(1:17000)。具体而言：

% 1. 首先使用缓存机制识别少数类样本
% 2. 针对少数类应用增强的数据增强
% 3. 在训练中结合类别权重和Focal Loss
% 4. 设置足够长的训练时间(至少75个epoch)以确保充分学习少数类特征

% 这种综合方法不仅提高了模型对少数类的识别能力，还保持了对主流类别的良好表现，实现了真正的平衡学习。

We compared the pros and cons of different class imbalance handling techniques:

\begin{table}
    \centering
    \caption{Comparison of Class Imbalance Handling Techniques}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Technique & Pros & Cons & Applicability \\
    \hline
    Class Weights & Simple implementation, direct handling of pixel-level imbalance & May lead to overfitting of minority classes & Moderately imbalanced datasets \\
    Focal Loss & Automatically focuses on hard-to-classify samples, no need for explicit weights & Hyperparameter (gamma) sensitivity & Scenarios with varying difficulty levels for different classes \\
    Data Augmentation & Increase diversity of minority class samples, improve generalization & Augmentation strategies need careful design & Scenarios with extremely few minority class samples \\
    Caching Mechanism & Avoids redundant calculations, improves training efficiency & Additional storage overhead & Large dataset training \\
    \hline
    \end{tabular}
\end{table}


**Best Solution**: Our experiments show that a multi-level approach combining all the above techniques is most effective, especially for the extreme imbalance problem (1:17000) in our dataset. Specifically:

\begin{enumerate}
    \item First, use a caching mechanism to identify minority class samples.
    \item Apply enhanced data augmentation for minority classes.
    \item Combine class weights and Focal Loss during training.
    \item Set a sufficiently long training time (at least 75 epochs) to ensure full learning of minority class features.
\end{enumerate}

This comprehensive approach not only improves the model's ability to recognize minority classes but also maintains good performance on mainstream classes, achieving true balanced learning.


\section{Discussion}
% discuss results and limitations
\subsection{Result Analysis}
This subsection provides an in-depth discussion of the experimental results, analyzing the strengths and weaknesses of the model.

\subsection{Limitations}
This subsection honestly points out the limitations of the research, such as dataset constraints, computational resource limitations, etc.

\subsection{Potential Applications}
This subsection explores the potential uses of the model in practical application scenarios, especially in the field of robotics.

\section{Conclusion}
% Lessons learned, future work
This section summarizes the main findings and contributions of the research. It reflects on the lessons learned throughout the research process and proposes possible future research directions and improvements.

% Keep the original references format and acknowledgment section
\section*{Acknowledgment}
Thanks to [relevant personnel/institutions] for their support of this research.

\begin{thebibliography}{00}
\bibitem{b0} [ResNet50 related citation]
\bibitem{b1} [YoloV3 related citation]
\bibitem{b2} [Deeplabv3 related citation]
\bibitem{b3} [CamVid dataset related citation]
\bibitem{b4} [Class imbalance problem related citation]
\bibitem{b5} [Semantic segmentation evaluation method related citation]
\end{thebibliography}

\end{document}